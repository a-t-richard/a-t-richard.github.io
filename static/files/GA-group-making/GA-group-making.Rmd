---
title: "Group making using genetic algorithm"
date: "2022-10-14"
output:
  html_document:
    preserve_yaml: true
    variant: gfm
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

## Introduction

Making relevant groups of students, for example to do some practical exercises
or in
[problem-based learning](https://teaching.cornell.edu/teaching-resources/engaging-students/problem-based-learning), 
is one common problem of teachers.
Groups must not be too small, or too large, and the mean "level" of each group
should be as homogeneous as possible.
We don't want one group with only students in difficulty and one group with only 
"good-grades" students.
In addition, students in each group should get along with each others,
and all participate to solve the exercice.

Generally, we just let the students make their own groups with some size
limitations, or we just do random groups.
Obviously, it's quite sub-optimal.

A possible way should be to use a metric (grades, IQ, et.) to evaluate
students and make groups according to this metric.
To illustrate this post, I decided to use the
["Reading the Mind in the Eyes" Test (RMET)](https://link.springer.com/referenceworkentry/10.1007/978-3-319-28099-8_549-1).
This test aims to evaluate the "emotional"/"social" intelligence of
the subjects.
According to [Riedl *et al.* (2021)](https://www.pnas.org/doi/full/10.1073/pnas.2005737118),
a high social perceptiveness of group members is quite corelated to
a high collective intelligence.
So, why not using RMET as a base to construct groups of students?

Let's first generate some random RMET scores to represent our students.
In my case, I'm teaching to 18-25 years old students at university.
According to [Kynast *et al.* (2021)](https://www.frontiersin.org/articles/10.3389/fnagi.2020.607107/full),
the mean RMET's score for this age range is 26
(with a standard deviation of 3.2).

```{r students}
nbStudents <- 20
seed <- 42

set.seed(seed)
students <- data.frame(rmets=round(rnorm(nbStudents, mean=26, sd=3.2)))
students
```

Now, we want to distribute these students into several groups in the
most homogeneous way possible, with a minimum and maximum group size
(defined below).

```{r setupMinMax}
minStudentsByGroup <- 3
maxStudentsByGroup <- 6

nbGroupsMax <- nbStudents %/% minStudentsByGroup
nbGroupsMin <- nbStudents %/% maxStudentsByGroup
```

This problem can be seen as a variation of the
[knapsack problem](https://en.wikipedia.org/wiki/Knapsack_problem).
However, instead of choosing wich item to put in a unique bag,
we want to take all the items and distribute them as homogeneously
as possible into several bags.
Knapsack problem is a
[NP-hard problem](https://en.wikipedia.org/wiki/NP-hardness).
We can then suppose that our variation of this problem is also
NP-hard.
Testing all the possible solutions is then not a option, it's too
time consuming.

A possible way is to use a
[metaheuristic algorithm](https://www.sciencedirect.com/science/article/pii/B9780128133149000104).
Meta-heuristic algorithms compose a subclass of algorithms
dedicated to explore the scope of possible solutions for
a specific problem. They based on heuristic on how to
explore solutions of problems in general, that why they are
called "meta-heuristic" algorithms.

One of these meta-heuristic algorithms is the Genetic Algorithm.
This algorithm, as we'll see in this blog post,
is quite adapted to explore solutions of
problems for which we have several parameters,
we can evaluate an output score and for which we want
to known the combinations of parameters that maximize
(or minimize) the output score.

In our case, we have several ways to group students and
we want find the most homogeneous one.
Therefore, using a genetic algorithm seems quite adapted.
It is also the occasion to test, and explain step by step, 
a genetic algorithm with a pratical use case.

## Naively apply basic genetic algorithm

To apply a basic genetic algorithm on our problem,
we need to define two things:

* the parameters that describe our problem, also called "solutions"
* our output score to evaluate a solution, also called "fitness"

The genetic algorithm 

### Representing our solutions

First, let's define how to represent a possible solution
of our problem.
In the case of a binary genetic algorithm, this means that each
parameter can only be set at 0 or 1, a solution of our
problem can be seen as a matrix.
Each row corresponding to a group and each column corresponding
to a student.
If a student is affected to a group, the corresponding cell
is set at 1. Else, the cell is set at 0.

```{r example}
example <- c(
  1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
  0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
  0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
)
matrix(example, ncol=nbStudents, byrow=TRUE)
```

However, the genetic algorithm we'll use doesn't use matrices
to describe solutions but lists. So, we'll have switch from
matrices to lists and from lists to matrices when necessary.

Besides, there are other ways to repersent our solutions.
I'll talk about it at the end of this post and explain
why I prefered to use this representation.

### Defining the score of a solution

Now, let's define our fitness function that will
evaluate the score of each solution explored by 
the genetic algorithm.

First, we want that a solution describe valid groups:
each student is associated to only one group, no group
is below the minimum number of students, and no group
is above the maximum number of students.
These are our "veto" conditions. If a solution
doesn't respect these conditions the score is
set a 0.

This can be evaluated as follow.

```{r veto-example}
# convert solution to matrix
m <- matrix(example, ncol=nbStudents, byrow=TRUE)

# get number of groups by student
nbGroupsByStudent <- colSums(m)

# get number of students by group
nbStudentsByGroup <- rowSums(m)

# remove groups with zero student
nbStudentsByGroup <- nbStudentsByGroup[nbStudentsByGroup != 0]

if(any(nbGroupsByStudent !=1) # If any student is associated to zero or several groups
   || any(nbStudentsByGroup < minStudentsByGroup) # or if any group is too small
   || any(nbStudentsByGroup > maxStudentsByGroup) # or if any group is too big
  )
  print("solution not valid")
#else
print("solution valid")
```

Once we have tested whether a solution is
valid, we want to evaluate its score.
In our case, we want to maximize the
"macro" RMET mean of the student groups.
In other words, we want to compute the
RMET means of each group, and then
compute the mean of these RMET means.

```{r rmetmean-example}
# to keep RMET means in memory
means <- c()

# for each possible group
for(i in 1:nbGroupsMax){
  
  # get the row corresponding to the group
  studentsInGroup <- m[i, ]
  
  # get the students' RMET of this group
  rmets <- students[studentsInGroup==1,]
  
  # if group not empty
  if(length(rmets) > 0){

    # compute the RMET mean of this group
    mean_rmet <- mean(rmets)
    print(paste("Group", i, ":",  mean_rmet))

    # and keep it in memory
    means[length(means)+1] <- mean_rmet
  
  }

}

# compute score as the "macro" mean of RMET means
score <- mean(means)
print(paste("Macro RMET mean :", score))
```

However, if we only focus on maximizing this "macro" RMET mean,
we could obtain solutions with one group $g$ a very high RMET mean score
while the other have poor RMET mean score.
This means that the group $g$ will be favorised in comparison to
the other.
This is not what we want.
We want that RMET scores are homogeneously dispatch amongs
groups and that the distribution of groups' RMET mean score
don't look like [actual salary distribution](todo).

To do so, we have to compute the standard deviation
of groups' RMET means.
Then, we want to minimize this standard deviation while
maximizing the "macro" RMET mean.
The best way to do it should be to compute these two
metrics and [pareto optimum](todo) solutions.
However, this imply to compute all the means and
standard deviations and then select pareto optimum
solutions.
Here, we want a unique fitness score that reflect
our goal.

An easy way to do it, simply consist to substract
the standard deviation of RMET means to the "macro"
RMET mean, as follow. This is generally called
a "penalty".

```{r penaltyexample}
penalty <- sd(means)
print(paste("Fitness =", score,"-",penalty,"=", score-penalty))
```

Now, we have all the elements to define our fitness
function.

```{r fitnessFunc}
fitness=function(solution)
{
  m <- matrix(solution, ncol=nbStudents, byrow=TRUE)

  nbGroupsByStudent <- colSums(m)
  nbStudentsByGroup <- rowSums(m)
  nbStudentsByGroup <- nbStudentsByGroup[nbStudentsByGroup != 0]
  
  if(any(nbGroupsByStudent != 1) 
     || any(nbStudentsByGroup < minStudentsByGroup) 
     || any(nbStudentsByGroup > maxStudentsByGroup))
    return(0)

  means <- c()
  for(i in 1:nbGroupsMax){
    studentsInGroup <- m[i, ]
    rmets <- students[studentsInGroup==1,]
    if(length(rmets) > 0){
      means[length(means)+1] <- mean(rmets)
    }
  }

  score <- mean(means)
  penalty <- sd(means)
  
  return(score - penalty)
}

fitness(example)
```

### Exploring solutions

Now we have defined our solutions and how to
evaluate them, we can apply genetic algorithm
on our problem.

To do so, we'll use the R package *GA* proposed by
(REF).
As input, the *ga* function needs to now:

* the type of genetic algorithm to apply ("binary", in our case)
* the number of parameters in a solution
  ($nbStudents \times nbGroupsMax$, in our case)
* the number of iterations, also called "generations"
* the number of solutions tested by iteration
* if we want to keep the best solution found in a generation N
  into the generation N+1, a process called "Elitism"

The steps of a genetic algorithm are quite simple:

1. a first sample of solutions are generate
2. the fitness of each solution is evaluated
3. two solutions are selected according to their
   fitness score, a step called "Selection"
4. these two solutions are mixed up together to
   generate two new solutions, a step called "cross-over"
5. step 3. and 4. are repeated until a new sample
   of solutions is generated
6. repeat the process from step 2. until the
   desired number of iteration is reached

Let's then explore solutions of our problem with
genetic algorithm.

```{r test-fitnessFunc}
library(GA)

GA=ga(
  type='binary',
  fitness=fitness,
  nBits=nbStudents*nbGroupsMax,
  maxiter=10,
  popSize=100,
  seed=seed,
  keepBest=TRUE,
  monitor = FALSE
)

plot(GA, ylim=c(0, max(students)))
```

Aaaaaand, it seems that it that it doesn't work.
Even with a high number of solutions and generations,
naively applying basic genetic algorithm on our problem 
seems to not even being able to find
valid solutions.

### Why it doesn’t work?

```{r nbComb}
library(gtools)
library(collections)


# To keep the combinations found for later
diffGroupSizesCombinations <- dict()
k <- 1 # to count the number of combinations found

# to define the different group size possible
diffGroupSize <- minStudentsByGroup:maxStudentsByGroup
totNbCombinations <- 0 # to count the total number of valid combinations

# for each number of group possible
for(i in nbGroupsMin:nbGroupsMax){
  
  # compute the combinations of i groups
  # from the different possible group sizes
  c <- combinations(
      n=length(diffGroupSize), 
      r=i,
      v = diffGroupSize,
      repeats.allowed = TRUE
    )
  
  # for each combination found
  for(j in 1:nrow(c)){
    cj <- c[j, ] # the combination
    
    # check if the sum of groups’ size
    # correspond to the number of students
    if(sum(cj) == nbStudents){
      print(cj)
      
      # keep the combination for later
      diffGroupSizesCombinations$set(as.character(k), cj)
      k = k + 1
      
      # compute the number of combination of students
      # for configuration of group sizes cj 
      nbCombinations <- 1.0
      nbStudentsLeft <- nbStudents
      # for each group size in cj configuration
      for(groupSize in cj){ 
        # compute combinations of group size
        # from students not grouped yet
        cGroupSize <- combinations(n=nbStudentsLeft, r=groupSize)
        
        # multiply with number of combinations found for previous groups
        nbCombinations = nbCombinations * nrow(cGroupSize)
        
        # update number of student not grouped yet
        nbStudentsLeft = nbStudentsLeft - groupSize
      }
      print(nbCombinations)
      
      # add number of combinations found for cj to the total
      totNbCombinations = totNbCombinations + nbCombinations
    }
  }
}

totNbCombinations
```

## Customizing GA to reduce solutions’ scope to only valid possibilities

TODO

### Initial population

```{r popFunc}
group_population <- function(object){
  # init population with empty matrix
  population <- matrix(
      rep(0, object@nBits*object@popSize),
      ncol=object@nBits,
      nrow=object@popSize
    )
  
  # generate chromosomes of each individual of the population
  for(i in 1:object@popSize){
    # choose one possible combination of group size
    k <- sample(1:diffGroupSizesCombinations$size(), 1)
    groupSizes <- diffGroupSizesCombinations$get(as.character(k))
    
    # for each group
    studentsNotGroupedYet <- 1:nbStudents
    for(j in 1:length(groupSizes)){
      # choose n students from students not grouped yet
      studentIds <- sample(
          studentsNotGroupedYet,
          groupSizes[j]
        )
      
      # for each student selected
      for(id in studentIds){
        # set student to group j
        population[i, nbStudents * (j-1) + id] = 1
        # remove student from students not grouped yet
        studentsNotGroupedYet = studentsNotGroupedYet[studentsNotGroupedYet != id]
      }
    }
  }
  
  return(population)
}
```

```{r test-popFunc}
GA=ga(
    type='binary',
    fitness=fitness,
    nBits=nbStudents*nbGroupsMax,
    population = group_population,
    maxiter=10,
    popSize=100,
    seed=seed,
    keepBest=TRUE,
    monitor = FALSE
  )

plot(GA, ylim=c(0, max(students)))
```

### Cross-over best solutions

```{r crossoverFunc}

group_crossover = function(object, parents){
  parentsFitness <- object@fitness[parents]
  parents <- object@population[parents,,drop = FALSE]
  n <- ncol(parents)
  
  children <- matrix(rep(0, 2*n), nrow = 2, ncol = n)
  childrenFitness <- rep(NA, 2)
  
  for(i in 1:nrow(parents)){
    # Select groups in parent 1
    parent1 <- parents[i,]
    groups <- matrix(parent1, ncol = nbStudents, byrow = TRUE)
    nbStudentsByGroup <- rowSums(groups)
    nbStudentsByGroup <- nbStudentsByGroup[nbStudentsByGroup != 0]
    nbGroupKept <- (length(nbStudentsByGroup) - 1) %/% 2
    groupKeptIds <- sample(1:length(nbStudentsByGroup), nbGroupKept, replace = FALSE)
    groupIdsLeft <- 1:length(nbStudentsByGroup)
    studentIdsLeft <-1:nbStudents
    for(groupKeptId in groupKeptIds){
      groupKept <- groups[groupKeptId,]
      studentIds <- which(groupKept == 1)
      for(studentId in studentIds){
        children[i, nbStudents * (groupKeptId-1) + studentId] = 1
        studentIdsLeft = studentIdsLeft[studentIdsLeft != studentId]
      }
      groupIdsLeft = groupIdsLeft[groupIdsLeft != groupKeptId]
    }
    
    # Select groups in parent 2 to keep their internal structures
    parent2 <- if(i==1) parents[2,] else parents[1,]
    groupsInParent2 <- matrix(parent2, ncol = nbStudents, byrow = TRUE)
    studentsByGroupsInParent2 <- rowSums(groupsInParent2)
    nbGroupsInParent2 <- length(studentsByGroupsInParent2[studentsByGroupsInParent2 != 0])
    
    while(length(groupIdsLeft) > 1){
      groupId <- sample(groupIdsLeft, 1)
      groupSize <- nbStudentsByGroup[groupId]
      while(groupSize > 0){
        groupIdInParent2 <- sample(1:nbGroupsInParent2, 1)
        studentIds <- which(groupsInParent2[groupIdInParent2, ] == 1)
        studentSelectedIds <- intersect(studentIds, studentIdsLeft)
        if(length(studentSelectedIds) > groupSize)
          studentSelectedIds <- sample(studentSelectedIds, groupSize, replace = FALSE)
        
        for(studentId in studentSelectedIds){
          children[i, nbStudents * (groupId-1) + studentId] = 1
          studentIdsLeft = studentIdsLeft[studentIdsLeft != studentId]
        }
        groupSize = groupSize - length(studentSelectedIds)
      }
      groupIdsLeft = groupIdsLeft[groupIdsLeft != groupId]
    }
    
    # Fill last group left with students left
    for(studentId in studentIdsLeft){
      groupId <- groupIdsLeft[1]
      children[i, nbStudents * (groupId-1) + studentId] = 1
    }
  }
  
  return(list(children=children, fitness=childrenFitness))
}

```

```{r test-coFunc}
GA=ga(
    type='binary',
    fitness=fitness,
    nBits=nbStudents*nbGroupsMax,
    population = group_population,
    crossover = group_crossover,
    maxiter=10,
    popSize=100,
    seed=seed,
    keepBest=TRUE,
    monitor=FALSE
  )

plot(GA, ylim=c(0, max(students)))
```

### Mutate solutions

```{r mutateFunction}
group_mutation <- function(object, parent){
  mutate <- parent <- as.vector(object@population[parent,])
  
  groups <- matrix(parent, ncol = nbStudents, byrow = TRUE)
  groups <- groups[rowSums(groups) != 0, ]
  
  selectedGroupIds <- sample(1:nrow(groups), 2, replace = FALSE)
  
  # select students to exchange
  group1 <- selectedGroupIds[1]
  student1 <- sample(which(groups[group1,] == 1), 1)
  group2 <- selectedGroupIds[2]
  student2 <- sample(which(groups[group2,] == 1), 1)
  
  # remove student from their groups
  mutate[nbStudents * (group1-1) + student1] = 0
  mutate[nbStudents * (group2-1) + student2] = 0
  
  # add them into the other groups
  mutate[nbStudents * (group1-1) + student2] = 1
  mutate[nbStudents * (group2-1) + student1] = 1
  
  return(mutate)
}
```

```{r test-mutateFunction}
GA=ga(
  type='binary',
  fitness=fitness,
  nBits=nbStudents*nbGroupsMax,
  population = group_population,
  crossover = group_crossover,
  mutation = group_mutation,
  maxiter=50,
  popSize=100,
  seed=seed,
  keepBest=TRUE,
  monitor=FALSE 
)
```

```{r plotGA}
plot(GA, ylim=c(0, max(students)))
```

```{r bestsol}
summary(GA)
```

## Visualize solutions


```{r test-igraph}
library(igraph)

display_groupmaking <- function(solution, fitness){
  g <- make_empty_graph(directed = FALSE)

  for(i in 1:nrow(students))
    g <- add_vertices(g, 1, label=paste(i, " (", students[i,], ")", sep=""), vertex.size=50)
  
  m <- matrix(solution, ncol = nbStudents, byrow = TRUE)
  rmet_means <- c()
  for(i in 1:nbGroupsMax){
    studentIds <- which(m[i, ] == 1)
    if(length(studentIds) > 0){
      rmet_means[length(rmet_means)+1] <- mean(students[m[i,]==1,])
      edges <- c()
      for(j in 1:length(studentIds)){
        if(j < length(studentIds)){
          edges[length(edges)+1] <- studentIds[j]
          edges[length(edges)+1] <- studentIds[j+1]
        }
        else{
          edges[length(edges)+1] <- studentIds[j]
          edges[length(edges)+1] <- studentIds[1]
        }
      }
      g <- add_edges(g, edges)
    }
  }
  
  # compute macro mean and standard deviation
  macro_rmet_mean <- mean(rmet_means)
  macro_rmet_sd <- sd(rmet_means)
  
  co <- layout_nicely(g)
  plot(0, 
       type="n",
       ann=TRUE, axes=FALSE,
       xlab="",
       ylab="",
       xlim=extendrange(co[,1]), 
       ylim=extendrange(co[,2]),
       main=paste("Score = ", fitness),
       sub=paste("Macro mean =", macro_rmet_mean, "±", macro_rmet_sd)
       )
  plot(g, layout=co, rescale=FALSE, add=TRUE,
       vertex.shape="rectangle",
       vertex.size=strwidth(V(g)$label) * 100,
       vertex.size2=strheight(V(g)$label) * 100 * 1.5,
       edge.width=5
       )
  
  
  ## add rmet’s mean for each group
  for(i in 1:nbGroupsMax){
    studentIds <- which(m[i, ] == 1)
    if(length(studentIds) > 0){
      studentCoors <- co[studentIds,]
      xcoor <- sum(studentCoors[,1]) / nrow(studentCoors)
      ycoor <- sum(studentCoors[,2]) / nrow(studentCoors)
      text(xcoor, ycoor, round(rmet_means[i], 1))
    }
  }
}
```

```{r displayBestSol}
for(i in 1:nrow(GA@solution)){
  display_groupmaking(GA@solution[i,], GA@fitnessValue)
}
```


## Conclusion

```{r test-genoud}
library("rgenoud")

func <- function(sol){
  
  # compute info for each group
  counts <- c()
  means <- c()
  for(i in 1:nbGroupsMax){
    if(length(sol[sol == i]) > 0){
      counts[length(counts) + 1] <- length(sol[sol == i])
      studentIds <- which(sol == i)
      means[length(means) + 1] <- mean(students[studentIds,])
    }
  }
  
  # check veto conditions
  if(any(counts < minStudentsByGroup) || any(counts > maxStudentsByGroup))
    return(0)
  
  # return scores - penalty
  return(mean(means) - sd(means))
}

mat <- matrix(
    rep(c(1,nbGroupsMax),nbStudents),
    nrow = nbStudents, 
    ncol = 2,
    byrow = TRUE
  )

genoudGA <- genoud(
    func,
    nvars = nbStudents,
    max = TRUE,
    pop.size = 100,
    max.generations = 50,
    hard.generation.limit = FALSE,
    Domains = mat,
    boundary.enforcement = 2,
    data.type.int = TRUE,
    print.level = 0
  )

genoudGA$value

genoudGA$par
```
