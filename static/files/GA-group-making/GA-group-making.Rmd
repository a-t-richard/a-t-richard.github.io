---
title: "Group making using genetic algorithm"
date: "2022-10-14"
output:
  html_document:
    preserve_yaml: true
    variant: gfm
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

## Introduction

Making relevant groups of students, to do some practical exercises for example
or in
[problem-based learning](https://teaching.cornell.edu/teaching-resources/engaging-students/problem-based-learning), 
is one common problem for teachers.
Groups must not be too small, or too large, and the mean "level" of each group
should be as homogeneous as possible.
We don't want one group with only students in difficulty and one group with only 
"good-grades" students.
In addition, students in each group should get along with each others,
and all participate to solve the exercises.

Generally, we just let the students make their own groups with some size
limitations, or we just do random groups.
Obviously, it's quite sub-optimal.

A possible way should be to use a metric (grades, IQ, etc.) to evaluate
students and make groups according to this metric.
To illustrate this post, I decided to use the
["Reading the Mind in the Eyes" Test (RMET)](https://link.springer.com/referenceworkentry/10.1007/978-3-319-28099-8_549-1).
This test aims to evaluate the "emotional"/"social" intelligence of
the subjects.
According to [Riedl *et al.* (2021)](https://www.pnas.org/doi/full/10.1073/pnas.2005737118),
a high social perceptiveness of group members is quite correlated to
a high collective intelligence.
So, why not using RMET as a base to construct groups of students?

Let's first generate some random RMET scores to represent our students.
In my case, I'm teaching to 18-25 years old students at university.
According to
[Kynast *et al.* (2021)](https://www.frontiersin.org/articles/10.3389/fnagi.2020.607107/full),
the mean RMET's score for this age range is 26
(with a standard deviation of 3.2).

```{r students}
nbStudents <- 20
seed <- 42

set.seed(seed)
students <- data.frame(rmets=round(rnorm(nbStudents, mean=26, sd=3.2)))
students
```

Now, we want to distribute these students into several groups in the
most homogeneous way possible, with a minimum and maximum number of students
by group.

```{r setupMinMax}
minStudentsByGroup <- 3
maxStudentsByGroup <- 6
```

Based on these minimum and maximum number of students by group, we can
easily now the minimum and maximum number of groups we can hypothetically
create.

```{r nbgroups}
# we round to the nearest value up with ceiling function
# because if nbStudents %% maxStudentsByGroup != 0, we need an additionnal group
nbGroupsMin <- ceiling(nbStudents / maxStudentsByGroup)
nbGroupsMin

# we don’t care of nbStudents %% minStudentsByGroup in this case
# because groups are not full
nbGroupsMax <- nbStudents %/% minStudentsByGroup
nbGroupsMax
```

This problem can be seen as a variation of the
[knapsack problem](https://en.wikipedia.org/wiki/Knapsack_problem).
However, instead of choosing wich item to put in a unique bag,
we want to take all the items and distribute them as homogeneously
as possible into several bags.
Knapsack problem is a
[NP-hard problem](https://en.wikipedia.org/wiki/NP-hardness).
We can then suppose that our variation of this problem is also
NP-hard.
Testing all the possible solutions is then not a option, it's too
time consuming.

A possible way is to use a
[metaheuristic algorithm](https://www.sciencedirect.com/science/article/pii/B9780128133149000104).
Meta-heuristic algorithms compose a subclass of algorithms
dedicated to explore the scope of possible solutions for
a specific problem. They based on heuristics on how to
explore solutions of problems in general, not heuristics to solve a specific
problem.
That why they are called "meta-heuristic" algorithms.

One of these meta-heuristic algorithms is the Genetic Algorithm.
It's also what we called a [bio-inspired algorithm](https://www.researchgate.net/profile/Hassan-Chizari/publication/341594685_Review_and_Classification_of_Bio-inspired_Algorithms_and_Their_Applications/links/61504a12d2ebba7be74b8163/Review-and-Classification-of-Bio-inspired-Algorithms-and-Their-Applications.pdf),
because it's inspired from the
[theory of evolution](https://en.wikipedia.org/wiki/Evolution)
in biology (aka: the most adapted to its environment will survive).
The genetic algorithm emulate the process of species selection
in biology, its main "meta-heuristic" being: it works for species
to find relevant adaptations to their environments, so why not
for algorithms to find relevant solutions to a problem?

The genetic algorithm, as we'll see in this blog post, is quite adapted
to explore solutions of problems for which we have several parameters,
we can evaluate an output score and for which we want
to known the combinations of parameters that maximize
(or minimize) the output score.

In our case, we have several ways to group students and
we want find the most homogeneous one.
Therefore, using a genetic algorithm seems quite adapted.
It is also the occasion to test, and explain step by step, 
a genetic algorithm with a practical use case.

## Naively apply basic genetic algorithm

To apply a basic genetic algorithm on our problem,
we need to define two things:

* the parameters that describe our problem, also called "solutions"
* our output score to evaluate a solution, also called "fitness"

The genetic algorithm will then explore possible solutions, evaluated
their fitness and try to find the most optimized solutions.

### Representing our solutions

First, let's define how to represent a possible solution
of our problem.
In the case of a binary genetic algorithm, this means that each
parameter can only be set at 0 or 1, a solution of our
problem can be seen as a matrix.
Each row corresponding to a group and each column corresponding
to a student.
If a student is affected to a group, the corresponding cell
is set at 1. Else, the cell is set at 0.

```{r example}
example <- c(
  1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
  0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
  0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
)
matrix(example, ncol=nbStudents, byrow=TRUE)
```

However, the genetic algorithm we'll use doesn't use matrices to describe
solutions but lists. So, we'll have switch from matrices to lists and from lists
to matrices when necessary.

Besides, there are other ways to represent our solutions. I'll talk about it at
the end of this post and explain why I preferred to use this representation.

### Defining the score of a solution

Now, let's define our fitness function that will evaluate the score of each
solution explored by the genetic algorithm.

First, we want that a solution describe valid groups: each student is associated
to only one group, no group is below the minimum number of students, and no
group is above the maximum number of students.
These are our "veto" conditions. If a solution doesn't respect these conditions
the score is set a 0.

This can be evaluated as follow.

```{r veto-example}
# convert solution to matrix
m <- matrix(example, ncol=nbStudents, byrow=TRUE)

# get number of groups by student
nbGroupsByStudent <- colSums(m)

# get number of students by group
nbStudentsByGroup <- rowSums(m)

# remove groups with zero student
nbStudentsByGroup <- nbStudentsByGroup[nbStudentsByGroup != 0]

if(any(nbGroupsByStudent != 1) # If any student is associated to zero or several groups
   || any(nbStudentsByGroup < minStudentsByGroup) # or if any group is too small
   || any(nbStudentsByGroup > maxStudentsByGroup)){ # or if any group is too big
  print("solution not valid")
} else {
  print("solution valid")
}
```

Once we have tested whether a solution is valid, we want to evaluate its score.

In our case, we want that the RMET mean score of each group is as similar to
each other to each other.
This way, no group will have an unfair advantage over the other groups.
We don’t want that the distribution of groups' RMET mean score
to be similar to the 
[current global wealth distribution](https://inequality.org/facts/global-inequality/).

So, first lets compute the RMET mean score of each group.

```{r rmetmean-example}
# to keep RMET means in memory
means <- c()

# for each possible group
for(i in 1:nbGroupsMax){
  
  # get the row corresponding to the group
  studentsInGroup <- m[i, ]
  
  # get the students' RMET of this group
  rmets <- students[studentsInGroup==1,]
  
  # if group not empty
  if(length(rmets) > 0){

    # compute the RMET mean of this group
    mean_rmet <- mean(rmets)
    print(paste("Group", i, ":",  mean_rmet))

    # and keep it in memory
    means[length(means)+1] <- mean_rmet
  
  }

}

# compute score as the "macro" mean of RMET means
score <- mean(means)
print(paste("Macro RMET mean :", score))
```

As we can observe in our example, if we only focus on mean RMET scores, we could
obtain solutions with one group (group 3) with an unfair advantage over the
other.
In fact, mathematically speaking, computing the mean of RMET means in our case
will always give us the same result.


To obtain RMET scores homogeneously dispatched among groups, it’s the
standard deviation between RMET mean by group that we want to minimize.
However, we’ll keep computing the "macro" mean of RMET means by group, 
for the skeptical ones, and we’ll subtract standard deviation of RMET means
by group to this "macro" mean.
This is generally called a "penalty".
This way, we have a maximum threshold to reach ("macro" RMET mean score) and the
genetic algorithm will search for solutions with a fitness score the closest
to this threshold.

(It’s also because I found it more satisfying to see a fitness growing that
reducing, it’s more "visual", you’ll see).

```{r penaltyexample}
penalty <- sd(means)
print(paste("Fitness =", score,"-",penalty,"=", score - penalty))
```

Now, we have all the elements to define our fitness function and test it
on our example.

```{r fitnessFunc}
fitness=function(solution)
{
  m <- matrix(solution, ncol=nbStudents, byrow=TRUE)

  nbGroupsByStudent <- colSums(m)
  nbStudentsByGroup <- rowSums(m)
  nbStudentsByGroup <- nbStudentsByGroup[nbStudentsByGroup != 0]
  
  if(any(nbGroupsByStudent != 1) 
     || any(nbStudentsByGroup < minStudentsByGroup) 
     || any(nbStudentsByGroup > maxStudentsByGroup))
    return(0)

  means <- c()
  for(i in 1:nbGroupsMax){
    studentsInGroup <- m[i, ]
    rmets <- students[studentsInGroup==1,]
    if(length(rmets) > 0){
      means[length(means)+1] <- mean(rmets)
    }
  }

  score <- mean(means)
  penalty <- sd(means)
  
  return(score - penalty)
}

fitness(example)
```

### Exploring solutions

Now we have defined our solutions and how to evaluate them, we can apply genetic
algorithm on our problem.

To do so, we'll use the R package [*GA*](https://luca-scr.github.io/GA/)
proposed by
[Scrucca (2013)](https://www.jstatsoft.org/article/view/v053i04).

As input, the *ga* function needs to now:

* the type of genetic algorithm to apply ("binary", in our case)
* the number of parameters in a solution
  ($nbStudents \times nbGroupsMax$, in our case)
* the number of iterations, also called "generations"
* the number of solutions tested by iteration
* if we want to keep the best solution found in a generation N
  into the generation N+1, a process called "Elitism"

The steps of a genetic algorithm are quite simple:

1. a first sample of solutions, a generation 0, are generate
2. the fitness of each solution is evaluated
3. two solutions are selected according to their fitness score, a step called
   "Selection"
4. these two solutions are mixed up together to generate two new solutions,
   a step called "cross-over"
5. step 3. and 4. are repeated until a new sample of solutions is generated
6. the process is repeated from step 2. until the desired number of iteration
   is reached

Let's then explore solutions of our problem with genetic algorithm.

```{r test-fitnessFunc}
library(GA)

GA=ga(
  type='binary',
  fitness=fitness,
  nBits=nbStudents*nbGroupsMax,
  maxiter=10,
  popSize=10,
  seed=seed,
  keepBest=TRUE,
  monitor = FALSE
)

plot(GA, ylim=c(0, mean(students$rmets)))
```

Aaaaaand, it doesn't work.

Even with a high number of solutions and generations, naively applying basic
genetic algorithm on our problem seems to not even being able to find valid
solutions.

### Why it doesn’t work?

There are two main reasons to this failure.
The first one is that the number of valid group combinations is largely lesser
than the number combinations tested by a basic binary genetic algorithm.

For instance, the number of valid group combinations can be calculated as
follow.

$$
\sum_{R \in \mathcal{R}} \prod_{i=0}^{|R|} C_{nbStudents - \sum_{j=0}^{i-1}r_j}^{r_i}
$$

With $\mathcal{R}$ the set of all possible combinations of group sizes
$R \in \mathcal{R}$, such as $\sum_{i=0}^{i=|R|} r_i = nbStudents$.
For example, the combination of group size $\{5,5,5,5,0,0\}$
is in $\mathcal{R}$.

Let's compute the number of valid group combinations, and intermediate number of
combinations, for our use case.

```{r nbComb}
library(gtools) # to use the 'combinations' function
library(collections) # to use the 'dict' structure

# To keep the combinations found for later
diffGroupSizesCombinations <- dict()
k <- 1 # to count the number of combinations found

# to define the different group size possible
diffGroupSize <- minStudentsByGroup:maxStudentsByGroup
totNbCombinations <- 0 # to count the total number of valid combinations

# for each number of group possible
for(i in nbGroupsMin:nbGroupsMax){
  
  # compute the combinations of i groups
  # from the different possible group sizes
  c <- combinations(
      n=length(diffGroupSize), 
      r=i,
      v = diffGroupSize,
      repeats.allowed = TRUE
    )
  
  # for each combination found
  for(j in 1:nrow(c)){
    cj <- c[j, ] # the combination
    
    # check if the sum of groups’ size
    # correspond to the number of students
    if(sum(cj) == nbStudents){
      print(cj)
      
      # keep the combination for later
      diffGroupSizesCombinations$set(as.character(k), cj)
      k = k + 1
      
      # compute the number of combination of students
      # for configuration of group sizes cj 
      nbCombinations <- 1.0
      nbStudentsLeft <- nbStudents
      # for each group size in cj configuration
      for(groupSize in cj){ 
        # compute combinations of group size
        # from students not grouped yet
        cGroupSize <- combinations(n=nbStudentsLeft, r=groupSize)
        
        # multiply with number of combinations found for previous groups
        nbCombinations = nbCombinations * nrow(cGroupSize)
        
        # update number of student not grouped yet
        nbStudentsLeft = nbStudentsLeft - groupSize
      }
      print(nbCombinations)
      
      # add number of combinations found for cj to the total
      totNbCombinations = totNbCombinations + nbCombinations
    }
  }
}

totNbCombinations
```

Let's now compare this number of valid group combinations with
the number of combinations tested by the genetic algorithm:

$$
2^{nbStudents \times nbGroupsMax}
$$

```{r totNbComb}
2^(nbStudents*nbGroupsMax)

totNbCombinations / 2^(nbStudents*nbGroupsMax)
```

We can observe that the number of valid group combinations is just a very small
fraction of all combinations tested by the genetic algorithm.
This means that the genetic algorithm only has a small chance to find a valid
group combination from random initial solutions.

In addition, and this is the second reason of this failure, the valid solution
are not "next to each other".
It's very easy to go from a valid solution to a non-valid one.
By simply changing a 0 to a 1, we can add a student to two groups, or create a 
too large group.
This means that, even with valid initial solutions, the
modifications made by the genetic algorithm to explore
possible solutions have a small chance to find another
valid solution.

## Customizing a Genetic Algorithm

Fortunately, the different steps of a genetic algorithm are highly customizable.
We'll then customize these different steps to reduce the scope of solutions
tested by the genetic algorithm to only valid groups combinations.

In the R package *GA*, we can customize the following steps:

* Initialization, to Generate an initial population with only valid solutions
* Selection, to select the best solutions for "reproduction"
  (We'll not customize this step. But, if you want to select
  [pareto optimum](https://en.wikipedia.org/wiki/Pareto_efficiency)
  solutions based on several fitness metrics, this is the place)
* Cross-over, to mixup characteristics of two valid
  solutions to create two new valid solutions
* Mutation, to change a characteristic of a valid solution
  to obtain a new valid solution

### Initial population

First of all, we need the genetic algorithm to start with only valid solutions.
To do so, we'll use the following function, based on the valid combinations of
group sizes computed before.

To create a valid initial solution, this function will first choose one random
combination $R$ of group sizes (for example $\{5, 5, 5, 5, 0, 0\}$).
And then, for each group size $r \in R$, the function choose $r$ students among
the students not grouped yet.

```{r popFunc}
group_population <- function(object){
  # init population with empty matrix
  population <- matrix(
      rep(0, object@nBits*object@popSize),
      ncol=object@nBits,
      nrow=object@popSize
    )
  
  # generate each individual of the population
  for(i in 1:object@popSize){
    # choose one possible combination of group size
    k <- sample(1:diffGroupSizesCombinations$size(), 1)
    groupSizes <- diffGroupSizesCombinations$get(as.character(k))
    
    # for each group
    studentsNotGroupedYet <- 1:nbStudents
    for(j in 1:length(groupSizes)){
      # choose n students from students not grouped yet
      studentIds <- sample(
          studentsNotGroupedYet,
          groupSizes[j]
        )
      
      # for each student selected
      for(id in studentIds){
        # set student to group j
        population[i, nbStudents * (j-1) + id] = 1
        # remove student from students not grouped yet
        studentsNotGroupedYet = studentsNotGroupedYet[studentsNotGroupedYet != id]
      }
    }
  }
  
  return(population)
}
```

Let's now test genetic algorithm with this custom initialization function.

```{r test-popFunc}
GA=ga(
    type='binary',
    fitness=fitness,
    nBits=nbStudents*nbGroupsMax,
    population = group_population,
    maxiter=10,
    popSize=100,
    seed=seed,
    keepBest=TRUE,
    monitor = FALSE
  )

plot(GA, ylim=c(0, mean(students$rmets)))
```

We can observe that the first generation of solutions, the one generated with
our custom function, are valid solutions.
However, we can also see that genetic algorithm directly generate non-valid solutions,
the median score being at zero.
This means that the solutions created from the initial solutions are non-valid
solutions and confirm our second assumption about why a basic binary genetic algorithm
failed to solve our problem.

### Cross-over best solutions

Now we have valid initial solutions, we need to find a way two combine the characteristics
of two "parents" valid solutions to generate two "children" solutions which:

* are also valid solutions
* keep sub-characteristics of the two "parents" solutions

This step is called "cross-over" and it's inspired from
[chromosomes' cross-over](https://en.wikipedia.org/wiki/Chromosomal_crossover)
(even if, in biology, cross-over occurs between two chromosomes of the same individual and
not between two chromosomes of two distincts individuals).
And, we'll see that, in our case, it's a tricky step to customize.

Let's take an example of two "parents" valid solutions for groups of 6 students, with minimum
2 students by group and 3 maximum.

```{r crossoverExample-step1}
ex_parent1 <- c(1,1,1,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0)
matrix(ex_parent1, ncol=6, byrow=TRUE)

ex_parent2 <- c(1,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,1,1)
matrix(ex_parent2, ncol=6, byrow=TRUE)
```

In a basic binary genetic algorithm, the "cross-over" function will generally split the
"parents" in two and generate the "children" with the four subparts.
Let's apply this process to our example.

```{r crossoverExample-step2}
split <- length(ex_parent1) %/% 2
ex_children1 <- append(ex_parent1[1:split], ex_parent2[(split+1):length(ex_parent1)])
matrix(ex_children1, ncol=6, byrow=TRUE)

ex_children2 <- append(ex_parent2[1:split], ex_parent1[(split+1):length(ex_parent1)])
matrix(ex_children2, ncol=6, byrow=TRUE)
```

We can observe that the generated "children" solutions, with this method, are not valid
at all.
This explain the direct decrease of fitness observed before.

So, to customize the customize the cross-over function we need to understand the
characteristics of a solution that we want "children" solutions inherit from "parents"
solutions.
In my opinion, we have two main characteristics of a solution:

* the number of students by group
* the sets of students that are grouped together

To simplify the cross-over process, let's keep the number of students from parents to
children (children1 will have the same number of students by group than parent1,
and so on for parent2).
This characteristic allow us to be sure to generate solutions with valid group sizes.

Concerning the sets of students grouped together, we can first observe that the
composition of the last group is implied by the other groups.
So we can focus on the composition $nbGroups - 1$ groups.

To keep a sub-part of students grouped together in the first parent, we can simply keep
the composition of $(nbGroups - 1) / 2$ random groups from this first parent.

Then, and its the tricky part, we have to get some characteristics from parent 2.
To do so, we can search, for each group $g$ not completed yet, for a subset of $|g|$
students that are grouped together in the second parent (or several subsets of
students grouped together in the second parent until obtaining $|g|$ students).

Finally, now we have $nbGroups - 1$ completed, we just have to complete the last
group with students not grouped yet.

In this way, we keep the subsets of students grouped together in both "parents",
while keeping a valid number of students by group.

This give us the following function.

```{r crossoverFunc}
group_crossover = function(object, parents){
  # get parents
  parents <- object@population[parents,,drop = FALSE]
  # get number of paremeters in parents
  n <- ncol(parents) 
  
  # initialize children with matrice of zeros
  children <- matrix(rep(0, 2*n), nrow = 2, ncol = n)
  # initialize children fitness at NA
  # (this will force GA function to compute fitness for the children)
  childrenFitness <- rep(NA, 2)
  
  # for each children to generate
  for(i in 1:nrow(children)){

    ## STEP 1: get group sizes from parent 1 ##

    # get parent1
    parent1 <- parents[i,]
    # get groups of students in parent1
    groups <- matrix(parent1, ncol = nbStudents, byrow = TRUE)

    # compute the number of students by groups
    nbStudentsByGroup <- rowSums(groups)
    # remove empty groups
    nbStudentsByGroup <- nbStudentsByGroup[nbStudentsByGroup != 0]
    
    ## STEP 2: get groups structure from parent 1  ##

    # compute the number of groups will keep from parent 1
    nbGroupKept <- (length(nbStudentsByGroup) - 1) %/% 2

    # choose random groups in parent1 to keep in child
    groupKeptIds <- sample(1:length(nbStudentsByGroup), nbGroupKept, replace = FALSE)
    
    # to trace the groups not keept from parent1
    groupIdsLeft <- 1:length(nbStudentsByGroup)

    # to trace the students not grouped yet
    studentIdsLeft <-1:nbStudents
    
    # for each group choosen in parent1
    for(groupKeptId in groupKeptIds){
      # get the corresponding row
      groupKept <- groups[groupKeptId,]
      # get the students in the group
      studentIds <- which(groupKept == 1)
      # for each student
      for(studentId in studentIds){
	      # set the corresponding cell at 1 in the child 
        children[i, nbStudents * (groupKeptId-1) + studentId] = 1
        # remove the student from the list of students not grouped yet
	      studentIdsLeft = studentIdsLeft[studentIdsLeft != studentId]
      }
      # remove group from groups not completed yet
      groupIdsLeft = groupIdsLeft[groupIdsLeft != groupKeptId]
    }
    

    ## STEP 3: get groups structure from parent 2 ##

    # get parent 2
    parent2 <- if(i==1) parents[2,] else parents[1,]
    groupsInParent2 <- matrix(parent2, ncol = nbStudents, byrow = TRUE)
    studentsByGroupsInParent2 <- rowSums(groupsInParent2)
    nbGroupsInParent2 <- length(studentsByGroupsInParent2[studentsByGroupsInParent2 != 0])
    
    # for the (nbStudents - 1) / 2 groups not completed yet
    while(length(groupIdsLeft) > 1){
      # choose a group
      groupId <- sample(groupIdsLeft, 1)
      
      # get the size of the group in parent 1
      groupSize <- nbStudentsByGroup[groupId]

      # while the corresponding group in child is not completed
      while(groupSize > 0){
	      # get a random group g2 in parent 2
        groupIdInParent2 <- sample(1:nbGroupsInParent2, 1)

      	# get the students in the group g2
        studentIds <- which(groupsInParent2[groupIdInParent2, ] == 1)

	      # get the intersection between g2 and the students not grouped yet in the child
        studentSelectedIds <- intersect(studentIds, studentIdsLeft)

	      # get a subset of this intersection if necessary
        if(length(studentSelectedIds) > groupSize)
          studentSelectedIds <- sample(studentSelectedIds, groupSize, replace = FALSE)
       
       # complete the child with students of the intersection	
        for(studentId in studentSelectedIds){
          children[i, nbStudents * (groupId-1) + studentId] = 1
          studentIdsLeft = studentIdsLeft[studentIdsLeft != studentId]
        }

	      # to know if we need more students to complete the group
        groupSize = groupSize - length(studentSelectedIds)
      }
      groupIdsLeft = groupIdsLeft[groupIdsLeft != groupId]
    }
    
    ## STEP 4: complete last group with students not grouped yet  ##
    for(studentId in studentIdsLeft){
      groupId <- groupIdsLeft[1]
      children[i, nbStudents * (groupId-1) + studentId] = 1
    }
  }
  
  return(list(children=children, fitness=childrenFitness))
}

```

Let's try genetic algorithm with this custom cross-over function, in combination
with our custom initialization function.

```{r test-coFunc}
GA=ga(
    type='binary',
    fitness=fitness,
    nBits=nbStudents*nbGroupsMax,
    population = group_population,
    crossover = group_crossover,
    maxiter=10,
    popSize=100,
    seed=seed,
    keepBest=TRUE,
    monitor=FALSE
  )

plot(GA, ylim=c(0, mean(students$rmets)))
```

We can observe that fitness scores do not directly decrease after the first generation.
Moreover, best solutions increase generation after generation, but mean fitness score still
decrease generation after generation.
This is due to the mutations (changing one parameter from 0 to 1, or from 1 to 0) of solutions
generated by our cross-over function.
As introduce before, in our problem it's easy to obtain a non-valid solution by simply
changing one parameter.

### Mutate solutions

The last step we need to custom to apply genetic algorithm on our problem is the mutation step.

The idea of this step is to apply, once children are created, a little modification to generate
more diversity in solutions tested.
Without mutation, there is a risk that genetic algorithm stay on local optimum solutions, even
with the cross-over step.

In a basic genetic algorithm, a mutation consist to change the value of one random parameter
(from 0 to 1, or from 1 to 0).
However, applying this kind of mutation on a valid solution of our problem can easily lead to
a non-valid solution.
For example, it could set a student to two different groups or to no group at all, or simply create
groups too small or too big.

In our case, the idea is simply choose two random students from two different groups
and exchange them.

To do so, we'll use the following function.
It simply choose two random groups, then one random student in each of these two groups.
Then, it update the solution to correspond to an exchange of the two selected students.

```{r mutateFunction}
group_mutation <- function(object, solution){
  # get the solution to mutate
  mutate <- solution <- as.vector(object@population[solution,])
  
  # transform to matrices to get groups
  groups <- matrix(solution, ncol = nbStudents, byrow = TRUE)
  # remove empty groups (useless)
  groups <- groups[rowSums(groups) != 0, ]
  
  # sample two random groups
  selectedGroupIds <- sample(1:nrow(groups), 2, replace = FALSE)
  
  # select students to exchange
  group1 <- selectedGroupIds[1]
  student1 <- sample(which(groups[group1,] == 1), 1)
  group2 <- selectedGroupIds[2]
  student2 <- sample(which(groups[group2,] == 1), 1)
  
  # remove student from their groups
  mutate[nbStudents * (group1-1) + student1] = 0
  mutate[nbStudents * (group2-1) + student2] = 0
  
  # add them into the other groups
  mutate[nbStudents * (group1-1) + student2] = 1
  mutate[nbStudents * (group2-1) + student1] = 1
  
  return(mutate)
}
```

Let's test genetic algorithm with this custom mutation function,
in combination with our initialization and cross-over functions.

```{r test-mutateFunction}
GA=ga(
  type='binary',
  fitness=fitness,
  nBits=nbStudents*nbGroupsMax,
  population = group_population,
  crossover = group_crossover,
  mutation = group_mutation,
  maxiter=300,
  popSize=100,
  seed=seed,
  keepBest=TRUE,
  monitor=FALSE 
)

plot(GA, ylim=c(0, mean(students$rmets)))
```

AAAND IT WORKS !

Moreover, if we summarize the results of genetic algorithm, several solutions
have been found.

```{r bestsol}
summary(GA)
```

However, sets of 0 and 1 are difficult to analyze.

## Visualize solutions

To better visualize the solutions we can plot them as graph/network of students.
To do so, we'll use the R package *igraph*.

```{r load-igraph}
library(igraph)
```

First, we need to represent the students with their RMETs scores.
Here, each student we'll be represented as nodes/vertices of the graph, with their 
id and RMET score as labels.

```{r plot-students}
g <- make_empty_graph(directed = FALSE)

for(i in 1:nrow(students))
  g <- add_vertices(g, 1, label=paste(i, " (", students[i,], ")", sep=""))


# plot graph
co <- layout_nicely(g)
plot(0, 
     type="n",
     ann=FALSE, axes=FALSE,
     xlim=extendrange(co[,1]), 
     ylim=extendrange(co[,2])
   )
plot(g, layout=co, rescale=FALSE, add=TRUE,
     vertex.shape="rectangle",
     vertex.size=strwidth(V(g)$label) * 100,
     vertex.size2=strheight(V(g)$label) * 100 * 1.5,
     edge.width=5
     )
```

Then, each edge of the graph will represent the fact that two students are in the same group.

```{r plot-groups}
solution <- GA@solution[1,]
m <- matrix(solution, ncol = nbStudents, byrow = TRUE)

# for each group
for(i in 1:nbGroupsMax){
  # get the students in the group
  studentIds <- which(m[i, ] == 1)
  if(length(studentIds) > 0){ # group not empty
    edges <- c()
    # for each student if the group
    for(j in 1:length(studentIds)){
      edges[length(edges)+1] <- studentIds[j]
      if(j < length(studentIds)) # if not last student
	# connect students j and j+#
        edges[length(edges)+1] <- studentIds[j+1]
      else # if last student, connect it to the first
        edges[length(edges)+1] <- studentIds[1]
    }
    g <- add_edges(g, edges)
  }
}

# plot graph
co <- layout_nicely(g)
plot(0, 
     type="n",
     ann=FALSE, axes=FALSE,
     xlim=extendrange(co[,1]), 
     ylim=extendrange(co[,2])
   )
plot(g, layout=co, rescale=FALSE, add=TRUE,
     vertex.shape="rectangle",
     vertex.size=strwidth(V(g)$label) * 100,
     vertex.size2=strheight(V(g)$label) * 100 * 1.5,
     edge.width=5
     )

```

Now, it could be interesting to plot the RMET mean score of each group.
To do so, we compute RMET mean and the center in the graph of each group.

```{r plot-rmet-means}
co <- layout_nicely(g)
plot(0, 
     type="n",
     ann=FALSE, axes=FALSE,
     xlim=extendrange(co[,1]), 
     ylim=extendrange(co[,2])
   )
plot(g, layout=co, rescale=FALSE, add=TRUE,
     vertex.shape="rectangle",
     vertex.size=strwidth(V(g)$label) * 100,
     vertex.size2=strheight(V(g)$label) * 100 * 1.5,
     edge.width=5
     )

rmet_means <- c()
for(i in 1:nbGroupsMax){
  studentIds <- which(m[i, ] == 1)
  if(length(studentIds) > 0){
    rmet_means[i] <- mean(students[m[i,]==1,])
    studentCoors <- co[studentIds,]
    xcoor <- sum(studentCoors[,1]) / nrow(studentCoors)
    ycoor <- sum(studentCoors[,2]) / nrow(studentCoors)
    text(xcoor, ycoor, round(rmet_means[i], 1))
  }
}
```

Finally, we can aggregate all these elements in a function to plot any solution.

```{r displaygroup-function}
display_groupmaking <- function(solution){
  g <- make_empty_graph(directed = FALSE)

  for(i in 1:nrow(students))
    g <- add_vertices(g, 1, label=paste(i, " (", students[i,], ")", sep=""), vertex.size=50)
  
  m <- matrix(solution, ncol = nbStudents, byrow = TRUE)
  rmet_means <- c()
  for(i in 1:nbGroupsMax){
    studentIds <- which(m[i, ] == 1)
    if(length(studentIds) > 0){
      rmet_means[i] <- mean(students[m[i,]==1,])
      edges <- c()
      for(j in 1:length(studentIds)){
	edges[length(edges)+1] <- studentIds[j]
        if(j < length(studentIds))
          edges[length(edges)+1] <- studentIds[j+1]
        else
          edges[length(edges)+1] <- studentIds[1]
      }
      g <- add_edges(g, edges)
    }
  }
  
  # compute macro mean and standard deviation
  macro_rmet_mean <- mean(rmet_means)
  macro_rmet_sd <- sd(rmet_means)
  
  co <- layout_nicely(g)
  plot(0, 
       type="n",
       ann=TRUE, axes=FALSE,
       xlab="",
       ylab="",
       xlim=extendrange(co[,1]), 
       ylim=extendrange(co[,2]),
       main=paste("Score = ", macro_rmet_mean - macro_rmet_sd),
       sub=paste("Macro mean =", macro_rmet_mean, "±", macro_rmet_sd)
       )
  plot(g, layout=co, rescale=FALSE, add=TRUE,
       vertex.shape="rectangle",
       vertex.size=strwidth(V(g)$label) * 100,
       vertex.size2=strheight(V(g)$label) * 100 * 1.5,
       edge.width=5
       )
  
  
  ## add rmet’s mean for each group
  for(i in 1:nbGroupsMax){
    studentIds <- which(m[i, ] == 1)
    if(length(studentIds) > 0){
      studentCoors <- co[studentIds,]
      xcoor <- sum(studentCoors[,1]) / nrow(studentCoors)
      ycoor <- sum(studentCoors[,2]) / nrow(studentCoors)
      text(xcoor, ycoor, round(rmet_means[i], 1))
    }
  }
}
```

Now, we can visualize the different solutions found by our custom genetic algorithm.

```{r displayBestSol}
for(i in 1:nrow(GA@solution)){
  display_groupmaking(GA@solution[i,])
}
```

We can observe that not all solutions have been found by the genetic algorithm.
Some students with the same RMET scores can be exchange.
But, it's not a problem because it give us a sample of good solutions to our problem.
Moreover, we can exchange some students if necessary, without losing score, to find
a configuration that satisfy everyone.

## Conclusion

I had multiple objectives with this blog post.
First, I wanted to practice genetic algorithms.
It's a type of algorithm I found very promising,
especially [genetic programming](https://www.sciencedirect.com/topics/medicine-and-dentistry/genetic-programming).
But, nowadays, I work almost entirely on machine learning
and deep learning.

Secondly, I wanted to find a way to create homogeneous student groups.
As I say in the introduction, it's a common problem for teachers
I have sometimes to deal with, and there is no real optimal solutions.
In addition, it was possible to apply genetic algorithm on
this problem, so it was the perfect occasion to test it.

Thirdly, I wanted to popularise the different how genetic algorithms
works with a pratical use case.
To do so, I tried to narrate the different steps I went through.
But, maybe, it would be more understandable with another use case.

Beside, I had to made some choices that could be not optimal.
For example, the choice of representing our problem as a
binary problem (I talk about that in the next section).
Also, in the cross-over I choosed to keep the configuration
of the parents' group size, which not a "bad" choice but
could decrease the diversity of group sizes in solutions.
Genetic algorithm will converge to a certain configuration of group sizes.
It could be interesting to create mutation functions that
could divide large groups or merge small groups.

More generally, as I said in the introduction, genetic algorithms are
quite interesting to find a set of parameters that
optimize the result of fitness function.
For example, genetic algorithms are sometimes used 
in machine learning to find the best structures
of neural networks, with a method called [NEAT](https://towardsdatascience.com/neat-an-awesome-approach-to-neuroevolution-3eca5cc7930f)
(the number of neurons, the number of layers, etc. can be considered
as parameters to optimize).

Beside, genetic algorithms are also interesting to
find solutions to multi-objectives problems.
As I briefly said when I presented the customizable steps
of genetic algorithms, it's possible to have several
fitness scores to evaluate a solution and look
for pareto optimal solutions at the selection phase.
It's more appropriate than trying to do some weighted average, which
is generally worst things to do, especially if the different
objectives have nothing in common and cannot compensate
each other.
Unfortunately, I didn't have the occasion to try genetic algorithm
on multi-objectives problems with our use case.
So, may be another time.

To conclude, I hope this blog post helped the few people
who read it to better understand genetic algorithms
and how to use/customize them.

### Bonus: another way to solve our problem with Genetic Algorithm

```{r genoud-example}
variation_example <- c(1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5)
```

$$
nbGroupsMax^{nbStudents}
$$
```{r genoud-combinations}
nbGroupsMax^nbStudents

totNbCombinations / nbGroupsMax^nbStudents
```

```{r genoud-fitness}
variation_fitness <- function(solution){
  
  # compute info for each group
  counts <- c()
  means <- c()
  for(i in 1:nbGroupsMax){
    if(length(solution[solution == i]) > 0){
      counts[length(counts) + 1] <- length(solution[solution == i])
      studentIds <- which(solution == i)
      means[length(means) + 1] <- mean(students[studentIds,])
    }
  }
  
  # check veto conditions
  if(any(counts < minStudentsByGroup) || any(counts > maxStudentsByGroup))
    return(0)
  
  # return scores - penalty
  return(mean(means) - sd(means))
}

variation_fitness(variation_example)
```

```{r test-genoud}
library("rgenoud")

mat <- matrix(
    rep(c(1,nbGroupsMax),nbStudents),
    nrow = nbStudents, 
    ncol = 2,
    byrow = TRUE
  )

variation_GA <- genoud(
    variation_fitness,
    nvars = nbStudents,
    max = TRUE,
    pop.size = 100,
    max.generations = 300,
    hard.generation.limit = FALSE,
    Domains = mat,
    boundary.enforcement = 2,
    data.type.int = TRUE,
    print.level = 0
  )

variation_GA$value

variation_GA$par
```
