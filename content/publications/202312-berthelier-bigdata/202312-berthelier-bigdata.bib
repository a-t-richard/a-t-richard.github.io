@inproceedings{berthelier:hal-04299405,
  TITLE = {{Toward training NLP models to take into account privacy leakages}},
  AUTHOR = {Berthelier, Gaspard and Richard, Antoine and Boutet, Antoine},
  URL = {https://hal.science/hal-04299405},
  BOOKTITLE = {BigData 2023},
  ADDRESS = {Sorrento, Italy},
  YEAR = {2023},
  MONTH = {Dec},
  KEYWORDS = {NLP models ; Privacy ; Membership Inference ; Counterfactual Memorisation ; Data Extraction},
  PDF = {https://hal.science/hal-04299405v1/file/NLP_Privacy_Hopitaux.pdf},
  HAL_ID = {hal-04299405},
  HAL_VERSION = {v1},
  abstract = {With the rise of machine learning and data-driven models especially in the field of Natural Language Processing (NLP), a strong demand for sharing data between organisations has emerged. However datasets are usually composed of personal data and thus subject to numerous regulations which require anonymization before disseminating the data. In the medical domain for instance, patient records are extremely sensitive and private, but the de-identification of medical documents is a complex task. Recent advances in NLP models have shown encouraging results in this field, but the question of whether deploying such models is safe remains. In this paper, we evaluate three privacy risks on NLP models trained on sensitive data. Specifically, we evaluate counterfactual memorization, which corresponds to rare and sensitive information which has too much influence on the model. We also evaluate membership inference as well as the ability to extract verbatim training data from the model. With this evaluation, we can cure data at risk from the training data and calibrate hyper parameters to provide a supplementary utility and privacy tradeoff to the usual mitigation strategies such as using differential privacy. We exhaustively illustrate the privacy leakage of NLP models through a use-case using medical texts and discuss the impact of both the proposed methodology and mitigation schemes.}
}
