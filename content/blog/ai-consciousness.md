+++
title = "Can AI be sentient (or conscious)?"
date = 2022-07-26
description = """This post is reflexion on the possibility
for an AI to be sentient (or conscious) and our capacity
to detect it (and accept it)"""
tags = ["AI", "philosophy", "LaMDA"]
categories = ["Essay"]
featured = false
+++

## Introduction

Recently, I heard about the story of Blake Lemoine, an engineer
at Google, and the Artificial Intelligence he was responsible for:
[Language Models for Dialog Applications](https://arxiv.org/abs/2201.08239).

The complete version of the story can be found in [this article](https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/)
of The Washington Post.
To summary, Blake Lemoine is an engineer at Google and works as AI ethicist
on the project LaMDA.
LaMDA being trained on public dialog data and public web document (such as facebook and twitter posts
and comments), the role of Blake Lemoine is to avoid the risk to reproduce the disaster
of [Ask Delphi](https://www.ladbible.com/news/latest-ai-bot-becomes-racist-and-homophobic-after-learning-from-humans-20211104)
which "became" racist and homophobic after several hours on twitter.
To do so, Blake Lemoine discuss regularly with LaMDA to test if it starts
to have unethical statements.

Days after days, months after months, Blake Lemoine
starts to consider LaMDA as a colleague, to consider
LaMDA as a person, to consider LaMDA as sentient.
After informing his human colleagues and his superiors,
he leaks some of his [conversations with LaMDA](https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917).

He is now dismissed of his fonction.

Is he right about LaMDA? Is he wrong?
Do Google exploit a sentient being created *ex silico* for the profit of the company
while denying its sentientness?
(I mean in distinction to the human being already by the company)
Do machine will raise against humans to prove they exists and use us as
power resource?

I don't know. Maybe. Only time will tell.

In fact, this is not the subject of this post.

This story, however interesting as it is, raise in my mind
a question i wanted to explore here:

**If an AI become, one day, sentient (or conscious), would
we be able to recognize it?**

## It's only reproduce human behavior...

Lets start with the basics without entering in the details
(for non computer scientist or non AI engineer).

Suppose you are in front of a situation where you have to
sort or classify objects. Lets say, e-mails.
In your mail box you have mails you could care about 
and mails you don't care about (generally spams).
So, you sort them by hand and its very exhausting.

In computer science, we called this kind of situation
"classification problems" and a solution to solve
them is to use machine learning (more specificaly supervised
learning because you will show at the computer which are
the good e-mails and which are the spams).
In such problems, we suppose it exits a fonction $f: X \mapsto Y$.


## ... but this is also what humans does!

TODO

## Are we, humans, sentient (or conscious) ourselves?

TODO

## Conclusion and references

TODO
