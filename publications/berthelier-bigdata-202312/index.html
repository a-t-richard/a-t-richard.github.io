<!DOCTYPE html>
<html lang="en">

<head>
  
  <link rel="shortcut icon" href="https:&#x2F;&#x2F;a-t-richard.github.io&#x2F;processed_images&#x2F;a8cbdf2e7fa4c9a100.png" type="image/png">
  <link rel="icon" href="https:&#x2F;&#x2F;a-t-richard.github.io&#x2F;processed_images&#x2F;a8cbdf2e7fa4c9a100.png" type="image/png">

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  <meta charset="utf-8">

  <!-- Custom header for users, includes custom css or js here -->
  

<title> Antoine Richard</title>

  
<!-- CSS -->
<link rel="stylesheet" href="https://a-t-richard.github.io/styles/styles.css" />

    <title>Publications |  Antoine Richard</title>
</head>

<body class="bg-white">
  <!-- Top nav bar -->
  
<nav id="header" class="w-full z-10 top-0 shadow-md mx-0">
  <div id="progress" class="top-0"></div>
  <!-- <div class="w-full max-w-5xl mx-auto flex flex-wrap items-center justify-between mt-0 py-2  sm:bg-green-900 md:bg-red-900 lg:bg-blue-900 bg-yellow-900"> -->
  <div class="w-full max-w-4xl mx-auto flex sm:flex-nowrap flex-wrap items-center justify-between mt-0 py-2">
    <div class="pl-4">
      <a class="text-gray-900 text-base no-underline hover:no-underline font-extrabold" href="https:&#x2F;&#x2F;a-t-richard.github.io/">
        Antoine Richard
      </a>
    </div>

    <div class="w-full flex-grow sm:flex sm:items-center flex-wrap sm:flex-nowrap sm:w-auto mt-2 sm:mt-0 bg-transparent z-20" id="nav-content">
      <ul class="list-reset flex flex-wrap sm:justify-end flex-1 items-center">
          <li class="mr-3 text-sm">
              <a href="https://a-t-richard.github.io/blog" class="inline-block text-gray-600 no-underline hover:text-gray-900 hover:text-underline py-2 px-4">
                Blog
              </a>
            </li>
            <li class="mr-3 text-sm">
              <a href="https://a-t-richard.github.io/publications" class="inline-block py-2 px-4 text-sky-600 font-bold no-underline">
                Publications
              </a>
            </li>
            <li class="mr-3 text-sm">
              <a href="https://a-t-richard.github.io/teaching" class="inline-block text-gray-600 no-underline hover:text-gray-900 hover:text-underline py-2 px-4">
                Teaching
              </a>
            </li>
            <li class="mr-3 text-sm">
            <a href=https://a-t-richard.github.io/#contacts class="inline-block text-gray-600 no-underline hover:text-gray-900 hover:text-underline py-2 px-4">
              Contact
            </a>
          </li>
          </ul>
    </div>
  </div>
</nav>

  <!-- Container -->
  


  <div class="container max-w-3xl mx-auto px-4">
  <div class="pt-8 flex-col mb-8">
    <h1 class="grow font-bold font-sans break-normal text-gray-900 text-3xl">Toward training NLP models to take into account privacy leakages
</h1>
    <div class="grow">
      <p class="text-sm text-slate-400">Gaspard Berthelier, Antoine Richard, Antoine Boutet
  </p>
    </div>
    <div class="grow">
      <p class="text-sm text-slate-400">
  BigData 2023, December 2023</p>
    </div>
    <div class="grow pt-2"> 
      
      <a class="rounded-md border-solid border border-blue-600 inline-flex items-center text-blue-600 justify-center px-2 py-1 mr-2 text-xs font-bold" target="_blank" href="https:&#x2F;&#x2F;hal.science&#x2F;hal-04299405&#x2F;document"> pdf </a>
      


      

      
      <a class="rounded-md border-solid border border-blue-600 inline-flex items-center text-blue-600 justify-center px-2 py-1 mr-2 text-xs font-bold" href="https:&#x2F;&#x2F;hal.science&#x2F;hal-04299405"> url </a>
      
    </div>
  </div>

  <p class="font-bold">Abstract</p>
  <article class="prose prose-indigo max-w-3xl pb-4">With the rise of machine learning and data-driven models especially in the field of Natural Language Processing (NLP), a strong demand for sharing data between organisations has emerged. However datasets are usually composed of personal data and thus subject to numerous regulations which require anonymization before disseminating the data. In the medical domain for instance, patient records are extremely sensitive and private, but the de-identification of medical documents is a complex task. Recent advances in NLP models have shown encouraging results in this field, but the question of whether deploying such models is safe remains. In this paper, we evaluate three privacy risks on NLP models trained on sensitive data. Specifically, we evaluate counterfactual memorization, which corresponds to rare and sensitive information which has too much influence on the model. We also evaluate membership inference as well as the ability to extract verbatim training data from the model. With this evaluation, we can cure data at risk from the training data and calibrate hyper parameters to provide a supplementary utility and privacy tradeoff to the usual mitigation strategies such as using differential privacy. We exhaustively illustrate the privacy leakage of NLP models through a use-case using medical texts and discuss the impact of both the proposed methodology and mitigation schemes.
  </article>
  <div>
    <p class="font-bold">Bibtex</p>
    <div class="prose prose-indigo pb-4">
      <pre ><code class="select-all">@inproceedings{berthelier:hal-04299405,
  TITLE = {{Toward training NLP models to take into account privacy leakages}},
  AUTHOR = {Berthelier, Gaspard and Richard, Antoine and Boutet, Antoine},
  URL = {https://hal.science/hal-04299405},
  BOOKTITLE = {BigData 2023},
  ADDRESS = {Sorrento, Italy},
  YEAR = {2023},
  MONTH = {Dec},
  KEYWORDS = {NLP models ; Privacy ; Membership Inference ; Counterfactual Memorisation ; Data Extraction},
  PDF = {https://hal.science/hal-04299405v1/file/NLP_Privacy_Hopitaux.pdf},
  HAL_ID = {hal-04299405},
  HAL_VERSION = {v1},
  abstract = {With the rise of machine learning and data-driven models especially in the field of Natural Language Processing (NLP), a strong demand for sharing data between organisations has emerged. However datasets are usually composed of personal data and thus subject to numerous regulations which require anonymization before disseminating the data. In the medical domain for instance, patient records are extremely sensitive and private, but the de-identification of medical documents is a complex task. Recent advances in NLP models have shown encouraging results in this field, but the question of whether deploying such models is safe remains. In this paper, we evaluate three privacy risks on NLP models trained on sensitive data. Specifically, we evaluate counterfactual memorization, which corresponds to rare and sensitive information which has too much influence on the model. We also evaluate membership inference as well as the ability to extract verbatim training data from the model. With this evaluation, we can cure data at risk from the training data and calibrate hyper parameters to provide a supplementary utility and privacy tradeoff to the usual mitigation strategies such as using differential privacy. We exhaustively illustrate the privacy leakage of NLP models through a use-case using medical texts and discuss the impact of both the proposed methodology and mitigation schemes.}
}
</code></pre>
    </div>
  </div>
  </div>
<!-- <div class="sticky bottom-0 px-3">
    <a href="#page-top">Back to top of page</a>
  </div> -->
  <footer class="bg-white">
    <div class="bg-white text-gray-400 container max-w-4xl mx-auto my-8 px-4">
    Â© Antoine Richard. Template <a class="underline" target="_blank" href="https://github.com/adfaure/kodama-theme">Kodama</a> made with <a class="underline" target="_blank" href="https://getzola.org">zola</a>, inspired by <a class="underline" target="_blank" href="https://wowchemy.com/"> wowchemy</a> academic theme.
    </div>
  </footer>
  </body>

</html>
